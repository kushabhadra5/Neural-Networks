{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN on MNIST datasset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the first 6 images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x2000 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(20, 20))\n",
    "\n",
    "for i in range(6):\n",
    "    ax = fig.add_subplot(1, 6, i+1, xticks = [], yticks = [])\n",
    "    ax.imshow(X_train[i], cmap = 'gray')\n",
    "    ax.set_title(str(y_train[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processsing the input training image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')/255\n",
    "X_test = X_test.astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encoding the target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9 2 1 3 1 4]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "num_classes = 10\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "print(y_train[:10])\n",
    "print(y_test[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping the image before proceeding to neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image shape:  (28, 28, 1)\n",
      "X_train shape:  (60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "img_rows, img_cols, color_channel= 28, 28, 1\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], \n",
    "                          img_rows, \n",
    "                          img_cols, \n",
    "                          color_channel)\n",
    "X_test = X_test.reshape(X_test.shape[0], \n",
    "                        img_rows, \n",
    "                        img_cols, \n",
    "                        color_channel)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "print(\"Input image shape: \", input_shape)\n",
    "print(\"X_train shape: \", X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Convolution neural network model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (Conv2D, \n",
    "                                     MaxPooling2D, \n",
    "                                     Flatten, \n",
    "                                     Dense, \n",
    "                                     Dropout, \n",
    "                                     GlobalAveragePooling2D)\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating the the structure of CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_5 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 14, 14, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 14, 14, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 7, 7, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 3136)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                100384    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 119530 (466.91 KB)\n",
      "Trainable params: 119530 (466.91 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# CONV_1: add CONV layer with RELU activation and depth = 32 kernels\n",
    "model.add(Conv2D(filters =  32, \n",
    "                 kernel_size =(3, 3), \n",
    "                 padding='same',\n",
    "                 activation='relu',\n",
    "                 input_shape=(28,28,1)))\n",
    "# POOL_1: downsample the image to choose the best features\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# CONV_2: here we increase the depth to 64\n",
    "model.add(Conv2D(64, \n",
    "                 kernel_size = (3, 3),\n",
    "                 padding='same', \n",
    "                 activation='relu'))\n",
    "# POOL_2: more downsampling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# flatten since too many dimensions, we only want a classification output\n",
    "model.add(Flatten())\n",
    "\n",
    "# FC_1: fully connected to get all relevant data\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# FC_2: output a softmax to squash the matrix into output probabilities for the 10 classes\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compiling the created CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ModelCheckpoint` module in `tensorflow.keras` can be used to save the model or its weights at regular intervals during training. This can be useful for resuming training if the training is interrupted, or for comparing the performance of different models.\n",
    "\n",
    "To use the `ModelCheckpoint` module, you need to create an instance of the `ModelCheckpoint` class and pass it to the `fit()` method of the model. The ModelCheckpoint class has the following parameters:\n",
    "- filepath: The path to the file where the model or weights will be saved.\n",
    "- monitor: The metric to be monitored. The model or weights will be saved when the monitored metric improves.\n",
    "- save_best_only: Whether to only save the best model or weights.\n",
    "- save_weights_only: Whether to only save the weights.\n",
    "- verbose: The verbosity level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.05940, saving model to model.weights.best.hdf5\n",
      "938/938 - 23s - loss: 0.1785 - accuracy: 0.9456 - val_loss: 0.0594 - val_accuracy: 0.9813 - 23s/epoch - 25ms/step\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 2: val_loss improved from 0.05940 to 0.03467, saving model to model.weights.best.hdf5\n",
      "938/938 - 23s - loss: 0.0514 - accuracy: 0.9845 - val_loss: 0.0347 - val_accuracy: 0.9886 - 23s/epoch - 24ms/step\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.03467\n",
      "938/938 - 22s - loss: 0.0356 - accuracy: 0.9890 - val_loss: 0.0702 - val_accuracy: 0.9757 - 22s/epoch - 23ms/step\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.03467\n",
      "938/938 - 22s - loss: 0.0272 - accuracy: 0.9913 - val_loss: 0.0470 - val_accuracy: 0.9849 - 22s/epoch - 23ms/step\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.03467\n",
      "938/938 - 22s - loss: 0.0221 - accuracy: 0.9934 - val_loss: 0.0381 - val_accuracy: 0.9877 - 22s/epoch - 23ms/step\n",
      "Epoch 6/10\n",
      "\n",
      "Epoch 6: val_loss improved from 0.03467 to 0.02780, saving model to model.weights.best.hdf5\n",
      "938/938 - 22s - loss: 0.0172 - accuracy: 0.9949 - val_loss: 0.0278 - val_accuracy: 0.9915 - 22s/epoch - 23ms/step\n",
      "Epoch 7/10\n",
      "\n",
      "Epoch 7: val_loss improved from 0.02780 to 0.02752, saving model to model.weights.best.hdf5\n",
      "938/938 - 22s - loss: 0.0145 - accuracy: 0.9956 - val_loss: 0.0275 - val_accuracy: 0.9920 - 22s/epoch - 23ms/step\n",
      "Epoch 8/10\n",
      "\n",
      "Epoch 8: val_loss improved from 0.02752 to 0.02724, saving model to model.weights.best.hdf5\n",
      "938/938 - 22s - loss: 0.0114 - accuracy: 0.9966 - val_loss: 0.0272 - val_accuracy: 0.9919 - 22s/epoch - 23ms/step\n",
      "Epoch 9/10\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.02724\n",
      "938/938 - 21s - loss: 0.0091 - accuracy: 0.9974 - val_loss: 0.0310 - val_accuracy: 0.9919 - 21s/epoch - 22ms/step\n",
      "Epoch 10/10\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.02724\n",
      "938/938 - 21s - loss: 0.0080 - accuracy: 0.9976 - val_loss: 0.0314 - val_accuracy: 0.9915 - 21s/epoch - 22ms/step\n"
     ]
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='Chapter6.model.weights.best.hdf5', \n",
    "                               verbose=1,\n",
    "                               save_best_only=True)\n",
    "\n",
    "hist = model.fit(X_train, y_train, \n",
    "                 batch_size=64, epochs=10,\n",
    "                 validation_data=(X_test, y_test), \n",
    "                 callbacks=[checkpointer],\n",
    "                 verbose=2, \n",
    "                 shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the Model with the Best Classification Accuracy on the Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model.weights.best.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate the Classification Accuracy on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 99.1900%\n"
     ]
    }
   ],
   "source": [
    "# evaluate test accuracy\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "accuracy = 100*score[1]\n",
    "\n",
    "# print test accuracy\n",
    "print('Test accuracy: %.4f%%' % accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
